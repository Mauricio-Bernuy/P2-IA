{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (1.23.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sklearn in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sklearn) (1.1.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn->sklearn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn->sklearn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn->sklearn) (1.23.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (1.23.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (4.37.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from seaborn) (1.23.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from seaborn) (1.5.0)\n",
      "Requirement already satisfied: matplotlib>=3.1 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from seaborn) (3.6.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.1->seaborn) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.1->seaborn) (4.37.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas>=0.25->seaborn) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ssd\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.7->matplotlib>=3.1->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install sklearn\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Mappings\n",
    "| a | b | c | d | e | f | g | h | i | k  | l  | m  | n  | o  | p  | q  | r  | s  | t  | u  | v  | w  | x  | y  | z |\n",
    "|---|---|---|---|---|---|---|---|---|---|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|\n",
    "| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = np.array([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# con LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin, TransformerMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class KNN_LDA(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_comps=1, k=3):\n",
    "        self.n_comps = n_comps\n",
    "        self.k = k\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        if y is None:\n",
    "            raise ValueError('requires y to be passed, but the target y is None')\n",
    "\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.is_fitted_ = True\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        \n",
    "        self.es_lda_model_ = LDA(n_components=self.n_comps)\n",
    "        es_lda = self.es_lda_model_.fit_transform(X,y.ravel())\n",
    "        self.es_kd_tree_ = KNeighborsClassifier(n_neighbors=self.k)\n",
    "        self.es_kd_tree_.fit(es_lda, y.ravel())\n",
    "       \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        X = check_array(X)\n",
    "        es_tf_test = self.es_lda_model_.transform(X)\n",
    "\n",
    "        return (self.es_kd_tree_.predict(es_tf_test))\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "      es_tf_test = self.es_lda_model_.transform(X)\n",
    "      return(self.es_kd_tree_.predict_proba(es_tf_test))\n",
    "check_estimator(KNN_LDA())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin, TransformerMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class KNN_PCA(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_comps=1, k=3):\n",
    "        self.n_comps = n_comps\n",
    "        self.k = k\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        if y is None:\n",
    "            raise ValueError('requires y to be passed, but the target y is None')\n",
    "\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.is_fitted_ = True\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.es_PCA_model_ = PCA(n_components=self.n_comps)\n",
    "\n",
    "        es_PCA = self.es_PCA_model_.fit_transform(X,y.ravel())\n",
    "        # print(es_PCA)\n",
    "        self.es_kd_tree_ = KNeighborsClassifier(n_neighbors=self.k)\n",
    "        self.es_kd_tree_.fit(es_PCA, y.ravel())\n",
    "       \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        X = check_array(X)\n",
    "        es_tf_test = self.es_PCA_model_.transform(X)\n",
    "\n",
    "        return (self.es_kd_tree_.predict(es_tf_test))\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "      es_tf_test = self.es_PCA_model_.transform(X)\n",
    "      return(self.es_kd_tree_.predict_proba(es_tf_test))\n",
    "check_estimator(KNN_PCA())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todas las caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin, TransformerMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as PCA\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class KNN_FULL(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_comps=1, k=3):\n",
    "        self.n_comps = n_comps\n",
    "        self.k = k\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        if y is None:\n",
    "            raise ValueError('requires y to be passed, but the target y is None')\n",
    "\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.is_fitted_ = True\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        \n",
    "        # self.es_PCA_model_ = PCA(n_components=self.n_comps)\n",
    "        # es_PCA = self.es_PCA_model_.fit_transform(X,y.ravel())\n",
    "        self.es_kd_tree_ = KNeighborsClassifier(n_neighbors=self.k)\n",
    "        # self.es_kd_tree_.fit(es_PCA, y.ravel())\n",
    "        self.es_kd_tree_.fit(X, y.ravel())\n",
    "       \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        X = check_array(X)\n",
    "        # es_tf_test = self.es_PCA_model_.transform(X)\n",
    "\n",
    "        # return (self.es_kd_tree_.predict(es_tf_test))\n",
    "        return (self.es_kd_tree_.predict(X))\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "      # es_tf_test = self.es_PCA_model_.transform(X)\n",
    "      # return(self.es_kd_tree_.predict_proba(es_tf_test))\n",
    "      return(self.es_kd_tree_.predict_proba(X))\n",
    "check_estimator(KNN_FULL())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotear matrix de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_cnf_matrix(Y_test, Y_pred):\n",
    "  cnf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "  # f = sns.heatmap(cnf_matrix, annot=True, fmt='d')\n",
    "  cnf_matrix = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "  plt.rcParams['figure.figsize'] = (20, 10)\n",
    "  ax= plt.subplot()\n",
    "  sns.heatmap(cnf_matrix, annot=True, fmt='.2f', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "  ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "  ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "sign_df = pd.read_csv(\"dataset/sign_mnist_full.csv\")\n",
    "\n",
    "sign_X = sign_df.loc[:,  sign_df.columns != \"label\"].values\n",
    "sign_Y = sign_df.loc[:,[\"label\"]].values\n",
    "\n",
    "# sign_df\n",
    "# SS = StandardScaler()\n",
    "# x = SS.fit_transform(x)\n",
    "# x = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_df_train = pd.read_csv(\"dataset/sign_mnist_train.csv\")\n",
    "\n",
    "sign_X_train = sign_df_train.loc[:,  sign_df_train.columns != \"label\"].values\n",
    "sign_Y_train = sign_df_train.loc[:,[\"label\"]].values\n",
    "\n",
    "sign_df_test = pd.read_csv(\"dataset/sign_mnist_test.csv\")\n",
    "\n",
    "sign_X_test = sign_df_test.loc[:,  sign_df_test.columns != \"label\"].values\n",
    "sign_Y_test = sign_df_test.loc[:,[\"label\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modl = KNN_LDA(21,6)\n",
    "# modl.fit(X=x, y=y.ravel())\n",
    "# yxd = modl.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnf_matrix = confusion_matrix(y_test, yxd)\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # f = sns.heatmap(cnf_matrix, annot=True, fmt='d')\n",
    "# cnf_matrix = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = (20, 10)\n",
    "# ax= plt.subplot()\n",
    "# sns.heatmap(cnf_matrix, annot=True, fmt='.2f', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "# ax.set_title('Confusion Matrix'); \n",
    "# # ax.xaxis.set_ticklabels([\"neutral\", \"surprise\", \"sad\", \"disgust\", \"fear\", \"happy\", \"angry\"]); ax.yaxis.set_ticklabels([\"neutral\", \"surprise\", \"sad\", \"disgust\", \"fear\", \"happy\", \"angry\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFold CV with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementInclude import include\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score,roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def KFold_KNN(X_, Y_, k_splits_ = 10, n_comps_ = 20, k_ = 3):\n",
    "  kf = KFold(n_splits=k_splits_, shuffle=False)\n",
    "  precision = recall = f1 = AUC = 0\n",
    "  for train_index, test_index in kf.split(X_):\n",
    "    X_train, X_test = X_[train_index, :], X_[test_index, :] \n",
    "    Y_train, Y_test = Y_[train_index], Y_[test_index]\n",
    "    Scaler = StandardScaler()\n",
    "    X_train = Scaler.fit_transform(X_train)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "\n",
    "    X_test = Scaler.transform(X_test)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    model = KNN_LDA(n_comps_,k_)\n",
    "    model.fit(X=X_train, y=Y_train.ravel())\n",
    "    Y_predicted = model.predict(X_test)\n",
    "\n",
    "    print(Y_test.ravel())\n",
    "    print(Y_predicted)      \n",
    "    \n",
    "    # metrics\n",
    "    precision += precision_score(Y_test, Y_predicted, average=\"macro\")\n",
    "    recall += recall_score(Y_test, Y_predicted, average=\"macro\")\n",
    "    f1 += f1_score(Y_test, Y_predicted,  average=\"macro\")\n",
    "    AUC += roc_auc_score(Y_test, model.predict_proba(X_test),multi_class='ovr', average=\"macro\")\n",
    "\n",
    "  precision /= k_splits_\n",
    "  recall /= k_splits_\n",
    "  f1 /= k_splits_\n",
    "  AUC /= k_splits_\n",
    "\n",
    "  print(f\"Final metrics ({k_splits_}-Fold, {n_comps_}-LDA vectors, {k_}-NN search)\\n\", \n",
    "  \"Precision:\", \"{:.2f}\".format(precision),\n",
    "  \" | Recall:\", \"{:.2f}\".format(recall), \n",
    "  \" | F1:\", \"{:.2f}\".format(f1),\n",
    "  \" | AUC:\",\"{:.2f}\".format(AUC))\n",
    "\n",
    "  # plot_cnf_matrix(Y_test, Y_predicted)\n",
    "  return(pd.DataFrame(np.array([[precision, recall, f1, AUC, k_splits_, n_comps_, k_]]),\n",
    "                   columns=['Precision', 'Recall', 'F1', 'AUC', 'Folds', 'LDA Vectors', 'NN Search']))\n",
    "\n",
    "\n",
    "# KFold_KNN(sign_X, sign_Y, 4, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar tabla de comparacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21 24 19 ... 13 24  8]\n",
      "[24 23  7 ... 13 15  7]\n",
      "[13  0 14 ... 21 15  6]\n",
      "[14  0 14 ...  3  5 20]\n",
      "Final metrics (2-Fold, 1-LDA vectors, 1-NN search)\n",
      " Precision: 0.13  | Recall: 0.13  | F1: 0.13  | AUC: 0.55\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [251], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m vector \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m]:\n\u001b[0;32m      5\u001b[0m   \u001b[39mfor\u001b[39;00m K_ \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m]:\n\u001b[0;32m      6\u001b[0m     table_knn \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([table_knn, \n\u001b[1;32m----> 7\u001b[0m               KFold_KNN(sign_X, sign_Y, fold, vector, K_)],\n\u001b[0;32m      8\u001b[0m               axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [243], line 23\u001b[0m, in \u001b[0;36mKFold_KNN\u001b[1;34m(X_, Y_, k_splits_, n_comps_, k_)\u001b[0m\n\u001b[0;32m     20\u001b[0m X_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X_test)\n\u001b[0;32m     22\u001b[0m model \u001b[39m=\u001b[39m KNN_LDA(n_comps_,k_)\n\u001b[1;32m---> 23\u001b[0m model\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mX_train, y\u001b[39m=\u001b[39;49mY_train\u001b[39m.\u001b[39;49mravel())\n\u001b[0;32m     24\u001b[0m Y_predicted \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(Y_test\u001b[39m.\u001b[39mravel())\n",
      "Cell \u001b[1;32mIn [237], line 28\u001b[0m, in \u001b[0;36mKNN_LDA.fit\u001b[1;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mes_lda_model_ \u001b[39m=\u001b[39m LDA(n_components\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_comps)\n\u001b[1;32m---> 28\u001b[0m es_lda \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mes_lda_model_\u001b[39m.\u001b[39;49mfit_transform(X,y\u001b[39m.\u001b[39;49mravel())\n\u001b[0;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mes_kd_tree_ \u001b[39m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk)\n\u001b[0;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mes_kd_tree_\u001b[39m.\u001b[39mfit(es_lda, y\u001b[39m.\u001b[39mravel())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:870\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\discriminant_analysis.py:596\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcovariance_estimator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    592\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcovariance estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    593\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis not supported \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    594\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwith svd solver. Try another solver\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    595\u001b[0m         )\n\u001b[1;32m--> 596\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_solve_svd(X, y)\n\u001b[0;32m    597\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlsqr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    598\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_solve_lsqr(\n\u001b[0;32m    599\u001b[0m         X,\n\u001b[0;32m    600\u001b[0m         y,\n\u001b[0;32m    601\u001b[0m         shrinkage\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshrinkage,\n\u001b[0;32m    602\u001b[0m         covariance_estimator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcovariance_estimator,\n\u001b[0;32m    603\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\discriminant_analysis.py:493\u001b[0m, in \u001b[0;36mLinearDiscriminantAnalysis._solve_svd\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    491\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(fac) \u001b[39m*\u001b[39m (Xc \u001b[39m/\u001b[39m std)\n\u001b[0;32m    492\u001b[0m \u001b[39m# SVD of centered (within)scaled data\u001b[39;00m\n\u001b[1;32m--> 493\u001b[0m U, S, Vt \u001b[39m=\u001b[39m linalg\u001b[39m.\u001b[39;49msvd(X, full_matrices\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    495\u001b[0m rank \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(S \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol)\n\u001b[0;32m    496\u001b[0m \u001b[39m# Scaling of within covariance is: V' 1/S\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\linalg\\_decomp_svd.py:130\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39m# perform decomposition\u001b[39;00m\n\u001b[0;32m    127\u001b[0m u, s, v, info \u001b[39m=\u001b[39m gesXd(a1, compute_uv\u001b[39m=\u001b[39mcompute_uv, lwork\u001b[39m=\u001b[39mlwork,\n\u001b[0;32m    128\u001b[0m                       full_matrices\u001b[39m=\u001b[39mfull_matrices, overwrite_a\u001b[39m=\u001b[39moverwrite_a)\n\u001b[1;32m--> 130\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m:\n\u001b[0;32m    131\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSVD did not converge\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m info \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "table_knn = pd.DataFrame(columns=['Precision', 'Recall', 'F1', 'AUC', 'Folds', 'LDA Vectors', 'NN Search'])\n",
    "\n",
    "for fold in [2,4]:\n",
    "  for vector in [1]:\n",
    "    for K_ in [1]:\n",
    "      table_knn = pd.concat([table_knn, \n",
    "                KFold_KNN(sign_X, sign_Y, fold, vector, K_)],\n",
    "                axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Folds</th>\n",
       "      <th>LDA Vectors</th>\n",
       "      <th>NN Search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.129258</td>\n",
       "      <td>0.130116</td>\n",
       "      <td>0.129483</td>\n",
       "      <td>0.546151</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.127568</td>\n",
       "      <td>0.128249</td>\n",
       "      <td>0.127707</td>\n",
       "      <td>0.545178</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision    Recall        F1       AUC  Folds  LDA Vectors  NN Search\n",
       "1   0.129258  0.130116  0.129483  0.546151    4.0          1.0        1.0\n",
       "0   0.127568  0.128249  0.127707  0.545178    2.0          1.0        1.0"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_knn.sort_values('AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...1...2...3...\n",
      "\n",
      "Final metrics (4-Iterations, 0.5-Sample Ratio 8-LDA vectors, 3-NN search)\n",
      " Precision: 0.98  | Recall: 0.98  | F1: 0.98  | AUC: 1.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Iterations</th>\n",
       "      <th>Sample Size</th>\n",
       "      <th>LDA Vectors</th>\n",
       "      <th>NN Search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.980486</td>\n",
       "      <td>0.980454</td>\n",
       "      <td>0.980441</td>\n",
       "      <td>0.995732</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision    Recall        F1       AUC  Iterations  Sample Size  \\\n",
       "0   0.980486  0.980454  0.980441  0.995732         4.0          0.5   \n",
       "\n",
       "   LDA Vectors  NN Search  \n",
       "0          8.0        3.0  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample # for Bootstrap sampling\n",
    "\n",
    "def Bootstrap_KNN(X_, Y_, n_iterations = 10, sample_size = 0.50, n_comps_ = 20, k_ = 3):\n",
    "  n_size = int(X_.shape[0] * sample_size) #Size of sample, picking only 50% of the given data in every bootstrap sample\n",
    "  indexes = list(range(0, X_.shape[0]))\n",
    "  precision = recall = f1 = AUC = 0\n",
    "\n",
    "  for i in range(n_iterations):\n",
    "    train = resample(indexes, n_samples = n_size)\n",
    "    train_unique = np.unique(train)\n",
    "    test = [ele for ele in indexes if ele not in train_unique]\n",
    "\n",
    "    X_train = X_[train]\n",
    "    Y_train = Y_[train]\n",
    "    X_test = X_[test]\n",
    "    Y_test = Y_[test]\n",
    "\n",
    "    Scaler = StandardScaler()\n",
    "    X_train = Scaler.fit_transform(X_train)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = Scaler.transform(X_test)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    model = KNN_LDA(n_comps_,k_)\n",
    "    model.fit(X=X_train, y=Y_train.ravel())\n",
    "    Y_predicted = model.predict(X_test)\n",
    "\n",
    "    # metrics\n",
    "    precision += precision_score(Y_test, Y_predicted, average=\"macro\")\n",
    "    recall += recall_score(Y_test, Y_predicted, average=\"macro\")\n",
    "    f1 += f1_score(Y_test, Y_predicted,  average=\"macro\")\n",
    "    AUC += roc_auc_score(Y_test, model.predict_proba(X_test),multi_class='ovr',average='macro')\n",
    "    print(f\"{i}...\",end='')\n",
    "    \n",
    "  print(\"\\n\")\n",
    "  precision /= n_iterations\n",
    "  recall /= n_iterations\n",
    "  f1 /= n_iterations\n",
    "  AUC /= n_iterations\n",
    "  \n",
    "  print(f\"Final metrics ({n_iterations}-Iterations, {sample_size}-Sample Ratio {n_comps_}-LDA vectors, {k_}-NN search)\\n\", \n",
    "  \"Precision:\", \"{:.2f}\".format(precision),\n",
    "  \" | Recall:\", \"{:.2f}\".format(recall), \n",
    "  \" | F1:\", \"{:.2f}\".format(f1),\n",
    "  \" | AUC:\",\"{:.2f}\".format(AUC))\n",
    "\n",
    "  # plot_cnf_matrix(Y_test, Y_predicted)\n",
    "  return(pd.DataFrame(np.array([[precision, recall, f1, AUC, n_iterations, sample_size, n_comps_, k_]]),\n",
    "                   columns=['Precision', 'Recall', 'F1', 'AUC', 'Iterations', 'Sample Size', 'LDA Vectors', 'NN Search']))\n",
    "\n",
    "# con las caracterisitcas que le pases\n",
    "Bootstrap_KNN(sign_X, sign_Y,4,n_comps_=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_knn_bootstrap = pd.DataFrame(columns=['Precision', 'Recall', 'F1', 'AUC', 'Iterations', 'Sample Size', 'LDA Vectors', 'NN Search'])\n",
    "\n",
    "for iterations in [2,4]:\n",
    "  for sample in [0.5]:\n",
    "    for vector in [1]:\n",
    "      for K_ in [1]:\n",
    "        table_knn_bootstrap = pd.concat([table_knn_bootstrap, \n",
    "                  Bootstrap_KNN(sign_X, sign_Y, iterations, sample, vector, K_)],\n",
    "                  axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_knn_bootstrap.sort_values('AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap generalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample # for Bootstrap sampling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score,roc_auc_score\n",
    "\n",
    "\n",
    "def Bootstrap_CLF(X_, Y_, n_iterations = 10, sample_size = 0.50, model = None, reducer = None):\n",
    "  n_size = int(X_.shape[0] * sample_size) #Size of sample, picking only 50% of the given data in every bootstrap sample\n",
    "  indexes = list(range(0, X_.shape[0]))\n",
    "  precision = recall = f1 = AUC = 0\n",
    "\n",
    "  for i in range(n_iterations):\n",
    "    # Seleccion bootstrap\n",
    "    train = resample(indexes, n_samples = n_size)\n",
    "    train_unique = np.unique(train)\n",
    "    test = [ele for ele in indexes if ele not in train_unique]\n",
    "    X_train = X_[train]\n",
    "    Y_train = Y_[train]\n",
    "    X_test = X_[test]\n",
    "    Y_test = Y_[test]\n",
    "\n",
    "    # Normalizacion de datos\n",
    "    Scaler = StandardScaler()\n",
    "    X_train = Scaler.fit_transform(X_train)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = Scaler.transform(X_test)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "\n",
    "    # reducir dimensionalidad, mapea a n_comps_ vectores\n",
    "    # lda = LDA(n_components=n_comps_) # tambien se podria pasar como funcion\n",
    "    if reducer == None:\n",
    "      X_train_reducer = X_train\n",
    "      X_test_reducer = X_test\n",
    "    else:\n",
    "      X_train_reducer = reducer.fit_transform(X_train,Y_train.ravel())\n",
    "      X_test_reducer = reducer.transform(X_test)\n",
    "\n",
    "    # print(\"reduced to\",X_train_reducer.shape[0])\n",
    "    # fit, predict\n",
    "    model.fit(X_train_reducer, Y_train.ravel())\n",
    "    # print(\"model fitted\")\n",
    "\n",
    "    Y_predicted = model.predict(X_test_reducer)\n",
    "    # print(\"model predicted\")\n",
    "\n",
    "    # model = KNN_LDA(n_comps_,k_)\n",
    "    # model.fit(X=X_train, y=Y_train.ravel())\n",
    "    # Y_predicted = model.predict(X_test)\n",
    "    print(Y_test.shape)\n",
    "    print(Y_predicted.shape)\n",
    "    # metrics\n",
    "    precision += precision_score(Y_test, Y_predicted, average=\"macro\")\n",
    "    recall += recall_score(Y_test, Y_predicted, average=\"macro\")\n",
    "    f1 += f1_score(Y_test, Y_predicted,  average=\"macro\")\n",
    "    AUC += roc_auc_score(Y_test, model.predict_proba(X_test_reducer),multi_class='ovr',average='macro')\n",
    "    print(f\"{i}...\",end='')\n",
    "    \n",
    "  print(\"\\n\")\n",
    "  precision /= n_iterations\n",
    "  recall /= n_iterations\n",
    "  f1 /= n_iterations\n",
    "  AUC /= n_iterations\n",
    "  \n",
    "  print(f\"Final metrics ({n_iterations}-Iterations, {sample_size}-Sample Ratio)\\n\", \n",
    "  \"Precision:\", \"{:.2f}\".format(precision),\n",
    "  \" | Recall:\", \"{:.2f}\".format(recall), \n",
    "  \" | F1:\", \"{:.2f}\".format(f1),\n",
    "  \" | AUC:\",\"{:.2f}\".format(AUC))\n",
    "  # plot_cnf_matrix(Y_test, Y_predicted)\n",
    "  return(np.array([[precision, recall, f1, AUC]]))\n",
    "\n",
    "\n",
    "# KNeighborsClassifier(n_neighbors=self.k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21032, 1)\n",
      "(21032,)\n",
      "0...(21007, 1)\n",
      "(21007,)\n",
      "1..."
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "cols = ['Precision', 'Recall', 'F1', 'AUC', 'Validator(...)','Reductor(...)', 'Classifier(...)']\n",
    "table_res = pd.DataFrame(columns=cols)\n",
    "# K fold de hasta 5 okok\n",
    "\n",
    "# BOOTSTRAP\n",
    "\n",
    "# K neighbors\n",
    "for it in [5,10]:\n",
    "  for sample in [0.50]:\n",
    "    for vector in [1]:\n",
    "      for K_ in [1]:\n",
    "        res = Bootstrap_CLF(sign_X, sign_Y, \n",
    "                            n_iterations=it, \n",
    "                            sample_size=sample, \n",
    "                            model=KNeighborsClassifier(n_neighbors=K_),\n",
    "                            reducer=LDA(n_components=vector))\n",
    "        table_res = pd.concat([table_res, pd.DataFrame([np.append(res, \n",
    "                            [f\"Bootstrap(n_iterations={it}, sample_size={sample})\",\n",
    "                              f\"LDA(n_components={vector})\",\n",
    "                              f\"KNeighborsClassifier(n_neighbors={K_})\"])],\n",
    "                            columns=cols)],axis=0, ignore_index = True)\n",
    "\n",
    "# SVC\n",
    "for it in [5,10]:\n",
    "  for sample in [0.50, 0.80]:\n",
    "    for vector in [1,4,8]:\n",
    "      res = Bootstrap_CLF(sign_X, sign_Y, \n",
    "                          n_iterations=it, \n",
    "                          sample_size=sample, \n",
    "                          model=SVC(decision_function_shape='ovo', probability=True),\n",
    "                          reducer=LDA(n_components=vector))\n",
    "\n",
    "      table_res = pd.concat([table_res, pd.DataFrame([np.append(res, \n",
    "                          [f\"Bootstrap(n_iterations={it}, sample_size={sample})\",\n",
    "                            f\"LDA(n_components={vector})\",\n",
    "                            f\"SVC(decision_function_shape=ovo)\"])],\n",
    "                          columns=cols)],axis=0, ignore_index = True)\n",
    "\n",
    "      res = Bootstrap_CLF(sign_X, sign_Y, \n",
    "                          n_iterations=it, \n",
    "                          sample_size=sample, \n",
    "                          model=SVC(decision_function_shape='ovr', probability=True),\n",
    "                          reducer=LDA(n_components=vector))\n",
    "      table_res = pd.concat([table_res, pd.DataFrame([np.append(res, \n",
    "                          [f\"Bootstrap(n_iterations={it}, sample_size={sample})\",\n",
    "                            f\"LDA(n_components={vector})\",\n",
    "                            f\"SVC(decision_function_shape=ovr)\"])],\n",
    "                          columns=cols)],axis=0, ignore_index = True)\n",
    "\n",
    "      res = Bootstrap_CLF(sign_X, sign_Y, \n",
    "                          n_iterations=it, \n",
    "                          sample_size=sample, \n",
    "                          model=CalibratedClassifierCV(LinearSVC(max_iter=10000)),\n",
    "                          reducer=LDA(n_components=vector))\n",
    "      table_res = pd.concat([table_res, pd.DataFrame([np.append(res, \n",
    "                          [f\"Bootstrap(n_iterations={it}, sample_size={sample})\",\n",
    "                            f\"LDA(n_components={vector})\",\n",
    "                            f\"LinearSVC())\"])],\n",
    "                          columns=cols)],axis=0, ignore_index = True)\n",
    "\n",
    "# K neighbors\n",
    "for it in [5,10]:\n",
    "  for sample in [0.50, 0.80]:\n",
    "    for vector in [1,4,8]:\n",
    "      for depth in [5,10,20,None]:\n",
    "        res = Bootstrap_CLF(sign_X, sign_Y, \n",
    "                            n_iterations=it, \n",
    "                            sample_size=sample, \n",
    "                            model = DecisionTreeClassifier(criterion=\"gini\",max_depth=depth),\n",
    "                            reducer=LDA(n_components=vector))\n",
    "        table_res = pd.concat([table_res, pd.DataFrame([np.append(res, \n",
    "                            [f\"Bootstrap(n_iterations={it}, sample_size={sample})\",\n",
    "                              f\"LDA(n_components={vector})\",\n",
    "                              f\"DecisionTreeClassifier(criterion='gini',max_depth={depth})\"])],\n",
    "                            columns=cols)],axis=0, ignore_index = True)\n",
    "                            \n",
    "table_res\n",
    "table_res.to_csv(\"table_res_bootstrap_output.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Validator(...)</th>\n",
       "      <th>Reductor(...)</th>\n",
       "      <th>Classifier(...)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.13105947660560827</td>\n",
       "      <td>0.1323883762822453</td>\n",
       "      <td>0.13157274920334644</td>\n",
       "      <td>0.5473350458720655</td>\n",
       "      <td>Bootstrap(n_iterations=2, sample_size=0.5)</td>\n",
       "      <td>LDA(n_components=1)</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1273264273099003</td>\n",
       "      <td>0.12823251362433535</td>\n",
       "      <td>0.12761052104379067</td>\n",
       "      <td>0.5451648304917462</td>\n",
       "      <td>Bootstrap(n_iterations=4, sample_size=0.5)</td>\n",
       "      <td>LDA(n_components=1)</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.12407397058729186</td>\n",
       "      <td>0.1825117406775842</td>\n",
       "      <td>0.1335715513622901</td>\n",
       "      <td>0.8223139516954325</td>\n",
       "      <td>Bootstrap(n_iterations=2, sample_size=0.5)</td>\n",
       "      <td>LDA(n_components=1)</td>\n",
       "      <td>SVC(decision_function_shape=ovo)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.12251413928137853</td>\n",
       "      <td>0.17641109266482535</td>\n",
       "      <td>0.12092146699146264</td>\n",
       "      <td>0.8253984802628608</td>\n",
       "      <td>Bootstrap(n_iterations=2, sample_size=0.5)</td>\n",
       "      <td>LDA(n_components=1)</td>\n",
       "      <td>SVC(decision_function_shape=ovr)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Precision               Recall                   F1  \\\n",
       "0  0.13105947660560827   0.1323883762822453  0.13157274920334644   \n",
       "1   0.1273264273099003  0.12823251362433535  0.12761052104379067   \n",
       "2  0.12407397058729186   0.1825117406775842   0.1335715513622901   \n",
       "3  0.12251413928137853  0.17641109266482535  0.12092146699146264   \n",
       "\n",
       "                  AUC                              Validator(...)  \\\n",
       "0  0.5473350458720655  Bootstrap(n_iterations=2, sample_size=0.5)   \n",
       "1  0.5451648304917462  Bootstrap(n_iterations=4, sample_size=0.5)   \n",
       "2  0.8223139516954325  Bootstrap(n_iterations=2, sample_size=0.5)   \n",
       "3  0.8253984802628608  Bootstrap(n_iterations=2, sample_size=0.5)   \n",
       "\n",
       "         Reductor(...)                      Classifier(...)  \n",
       "0  LDA(n_components=1)  KNeighborsClassifier(n_neighbors=1)  \n",
       "1  LDA(n_components=1)  KNeighborsClassifier(n_neighbors=1)  \n",
       "2  LDA(n_components=1)     SVC(decision_function_shape=ovo)  \n",
       "3  LDA(n_components=1)     SVC(decision_function_shape=ovr)  "
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# contain_values = df[df['month'].str.contains('Ju')]\n",
    "\n",
    "table_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN con train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from distutils.command.bdist import show_formats\n",
    "# from xml.etree.ElementInclude import include\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import f1_score, recall_score, precision_score,roc_auc_score\n",
    "# from sklearn.metrics import classification_report\n",
    "# import numpy as np\n",
    "# import itertools\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def split_KNN(X_, Y_, k_splits_ = 10, n_comps_ = 20, k_ = 3):\n",
    "#   # kf = KFold(n_splits=k_splits_, random_state=42, shuffle=True)\n",
    "#   # kf = KFold(n_splits=k_splits_)\n",
    "\n",
    "#   # split = 0\n",
    "#   precision = recall = f1 = AUC = 0\n",
    "#   X_train, X_test, Y_train, Y_test = train_test_split(X_, Y_, test_size=0.25, \n",
    "#                                                       shuffle=True)\n",
    "#                                                       # random_state=42)\n",
    "#   # X_train, X_test, Y_train, Y_test = sign_X_train, sign_X_test,sign_Y_train,sign_Y_test\n",
    "\n",
    "\n",
    "#     # X_train, X_test = X_[train_index, :], X_[test_index, :] \n",
    "#     # Y_train, Y_test = Y_[train_index], Y_[test_index]\n",
    "\n",
    "#   Scaler = StandardScaler()\n",
    "#   X_train = Scaler.fit_transform(X_train)\n",
    "#   X_train = pd.DataFrame(X_train)\n",
    "\n",
    "#   X_test = Scaler.transform(X_test)\n",
    "#   X_test = pd.DataFrame(X_test)\n",
    "#   # print(X_test.head())\n",
    "#   # return \n",
    "#   model = KNN_LDA(n_comps_,k_)\n",
    "#   model.fit(X=X_train, y=Y_train.ravel())\n",
    "#   Y_predicted = model.predict(X_test)\n",
    "\n",
    "#   np.set_printoptions(threshold=300)\n",
    "#   print(Y_test.ravel())\n",
    "#   print(Y_predicted)      \n",
    "#   # metrics\n",
    "\n",
    "#   precision += precision_score(Y_test, Y_predicted, average=\"weighted\")\n",
    "#   recall += recall_score(Y_test, Y_predicted, average=\"weighted\")\n",
    "#   f1 += f1_score(Y_test, Y_predicted,  average=\"weighted\")\n",
    "#   AUC += roc_auc_score(Y_test, model.predict_proba(X_test),multi_class='ovr')\n",
    "\n",
    "#   # precision /= k_splits_\n",
    "#   # recall /= k_splits_\n",
    "#   # f1 /= k_splits_\n",
    "#   # AUC /= k_splits_\n",
    "\n",
    "#   print(f\"Final metrics ({0}-Fold, {n_comps_}-LDA vectors, {k_}-NN search)\\n\", \n",
    "#   \"Precision:\", \"{:.2f}\".format(precision),\n",
    "#   \" | Recall:\", \"{:.2f}\".format(recall), \n",
    "#   \" | F1:\", \"{:.2f}\".format(f1),\n",
    "#   \" | AUC:\",\"{:.2f}\".format(AUC))\n",
    "\n",
    "#   return(pd.DataFrame(np.array([[precision, recall, f1, AUC, k_splits_, n_comps_, k_]]),\n",
    "#                    columns=['Precision', 'Recall', 'F1', 'AUC', 'Folds', 'LDA Vectors', 'NN Search']))\n",
    "\n",
    "#   # plot_cnf_matrix(Y_test, Y_predicted)\n",
    "\n",
    "\n",
    "# split_KNN(sign_X, sign_Y, 20, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xml.etree.ElementInclude import include\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import f1_score, recall_score, precision_score,roc_auc_score\n",
    "# from sklearn.metrics import classification_report\n",
    "# import numpy as np\n",
    "# import itertools\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def query_get_expressions(kd_tree, x_list_validation, y_list, k=3):\n",
    "#   d,i = kd_tree.query(x_list_validation,k)\n",
    "#   expression_list = y_list[i]\n",
    "\n",
    "#   return d, expression_list.flatten()\n",
    "\n",
    "# def startingKNN(X_, Y_, k_splits_ = 10, n_comps_ = 20, k_ = 3):\n",
    "#   # kf = KFold(n_splits=k_splits_, random_state=False, shuffle=False)\n",
    "  \n",
    "#   df = pd.read_csv(\"dataset/sign_mnist_train.csv\")\n",
    "#   df\n",
    "#   from sklearn.preprocessing import StandardScaler\n",
    "#   x = df.loc[:,  df.columns != \"label\"].values\n",
    "#   y = df.loc[:,[\"label\"]].values\n",
    "\n",
    "#   SS = StandardScaler()\n",
    "#   x = SS.fit_transform(x)\n",
    "#   x = pd.DataFrame(x)\n",
    "\n",
    "#   lda_model = LDA(n_components=21)\n",
    "#   lda = lda_model.fit_transform(x,y.ravel())\n",
    "  \n",
    "#   kd_tree = spatial.KDTree(lda)\n",
    "\n",
    "#   df_test = pd.read_csv(\"dataset/sign_mnist_test.csv\")\n",
    "#   df_test\n",
    "\n",
    "#   x_test = df_test.loc[:,  df_test.columns != \"label\"].values\n",
    "#   y_test = df_test.loc[:,[\"label\"]].values\n",
    "\n",
    "#   x_test = SS.transform(x_test)\n",
    "#   x_test = pd.DataFrame(x_test)\n",
    "#   # print(x_test.head())\n",
    "#   # return\n",
    "\n",
    "#   tf_test = lda_model.transform(x_test)\n",
    "\n",
    "#   y_pred_val = []\n",
    "#   for x_row in tf_test:\n",
    "#     # print(x_row)\n",
    "#     # tf = lda_model.transform([x_row])\n",
    "#     distances, expressions = query_get_expressions(kd_tree, x_row, y, 6)\n",
    "#     voted_expression = max(set(expressions), key=lambda x: list(expressions).count(x))\n",
    "#     y_pred_val.append(voted_expression)\n",
    "\n",
    "#   print(len(y_test))\n",
    "#   print(len(y_pred_val))\n",
    "\n",
    "#   precision = precision_score(y_test, y_pred_val, average=\"weighted\")\n",
    "#   recall = recall_score(y_test, y_pred_val, average=\"weighted\")\n",
    "#   f1 = f1_score(y_test, y_pred_val,  average=\"weighted\")\n",
    "# # tf_test.shape\n",
    "\n",
    "#   # precision /= k_splits_\n",
    "#   # recall /= k_splits_\n",
    "#   # f1 /= k_splits_\n",
    "#   # AUC /= k_splits_\n",
    "\n",
    "#   print(f\"Final metrics ({0}-Fold, {21}-LDA vectors, {6}-NN search)\\n\", \n",
    "#   \"Precision:\", \"{:.2f}\".format(precision),\n",
    "#   \" | Recall:\", \"{:.2f}\".format(recall), \n",
    "#   \" | F1:\", \"{:.2f}\".format(f1))\n",
    "  \n",
    "#   plot_cnf_matrix(y_test, y_pred_val)\n",
    "\n",
    "\n",
    "# startingKNN(sign_X, sign_Y, 20, 5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (Support Vector Machines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25970, 8)\n",
      "LinearSVC: \n",
      "0.7391057485130804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SSD\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def SVM_function(X_, Y_):\n",
    "  X_train, X_test, Y_train, Y_test = train_test_split(X_, Y_, test_size=0.25, \n",
    "                                                      shuffle=True)\n",
    "  #ovo (ne-versus-one)\n",
    "\n",
    "  #reducir dimensionalidad, mapea a 8 vectores\n",
    "  lda = LDA(n_components=8)\n",
    "  X_train_lda = lda.fit_transform(X_train,Y_train.ravel())\n",
    "  X_test_lda = lda.transform(X_test)\n",
    "\n",
    "  print(X_train_lda.shape)\n",
    "  \n",
    "  # #Entrenamiento del \n",
    "  # #ovo (one-versus-one)\n",
    "  # print(\"OneVsOne: \")\n",
    "  # clf = SVC(decision_function_shape='ovo')\n",
    "  # clf.fit(X_train_lda, Y_train.ravel())\n",
    "  # Y_predicted = clf.predict(X_test_lda)\n",
    "  # print(precision_score(Y_test, Y_predicted, average=\"macro\"))\n",
    "  \n",
    "  # #ovr (one-versus-rest)\n",
    "  # print(\"OneVsRest: \")\n",
    "  # clf2 = SVC(decision_function_shape='ovr')\n",
    "  # clf2.fit(X_train_lda, Y_train.ravel())\n",
    "  # Y_predicted2 = clf2.predict(X_test_lda)\n",
    "  # print(precision_score(Y_test, Y_predicted2, average=\"macro\"))\n",
    "\n",
    "  #LinearSVC\n",
    "  print(\"LinearSVC: \")\n",
    "  lin_clf = svm.LinearSVC()\n",
    "  lin_clf.fit(X_train_lda, Y_train.ravel())\n",
    "  Y_predicted_linear = lin_clf.predict(X_test_lda)\n",
    "  print(precision_score(Y_test, Y_predicted_linear, average=\"macro\"))\n",
    "\n",
    "SVM_function(sign_X, sign_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARBOLES DE DECISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score 0.8612534986935819\n",
      "f1_score 0.8609226131549891\n",
      "recall_score 0.8610190826864242\n",
      "roc_auc_score 0.9274861023117129\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, roc_auc_score, precision_score\n",
    "\n",
    "def decisionTree(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(\"precision_score\", precision_score(y_test, y_pred, average = 'macro'))\n",
    "    print(\"f1_score\", f1_score(y_test, y_pred, average = 'macro'))\n",
    "    print(\"recall_score\", recall_score(y_test, y_pred, average = 'macro'))\n",
    "    print(\"roc_auc_score\", roc_auc_score(y_test, clf.predict_proba(X_test), multi_class='ovr', average = 'macro'))\n",
    "    \n",
    "decisionTree(sign_X,sign_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [325], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     bool_list \u001b[39m=\u001b[39m [predict(trees,d) \u001b[39m==\u001b[39m d[sign_Y] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mto_dict(orient\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m     25\u001b[0m     \u001b[39mprint\u001b[39m(bool_list)\n\u001b[1;32m---> 27\u001b[0m f,d \u001b[39m=\u001b[39m random_forest(sign_Y,\u001b[39m5\u001b[39m)\n\u001b[0;32m     28\u001b[0m bootstrap(f,d)\n",
      "Cell \u001b[1;32mIn [325], line 5\u001b[0m, in \u001b[0;36mrandom_forest\u001b[1;34m(dataset, n)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandom_forest\u001b[39m(dataset, n):\n\u001b[0;32m      4\u001b[0m     forest \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m     test_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39mdataset\u001b[39m.\u001b[39;49mcolumns)\n\u001b[0;32m      6\u001b[0m     size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataset[sign_Y])\n\u001b[0;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "#Bootstrap\n",
    "\"\"\"import operator\n",
    "def random_forest(dataset, n):\n",
    "    forest = []\n",
    "    test_data = pd.DataFrame(columns=dataset.columns)\n",
    "    size = len(dataset[PREDICT])\n",
    "    for _ in range(n):\n",
    "        idx = np.sort(np.random.choice(size, int(0.7*size), replace=True))\n",
    "        n_idx = np.array([_ for _ in range(size) if _ not in idx])\n",
    "        #display(data.iloc[idx, :])\n",
    "        #display(data.iloc[n_idx, :])\n",
    "        tr = Tree(get_info_gain)\n",
    "        tr.train(dataset.iloc[idx, :]) \n",
    "        forest.append(tr)\n",
    "        test_data = pd.concat([test_data, dataset.iloc[n_idx, :]])\n",
    "    return forest, test_data.drop_duplicates()\n",
    "\n",
    "\n",
    "def predict(trees, row):\n",
    "    pred = [t.traversal(t.root,row) for t in trees]\n",
    "    return max(set(pred), key=pred.count)\n",
    "\n",
    "def bootstrap(trees, data):\n",
    "    bool_list = [predict(trees,d) == d[PREDICT] for d in data.to_dict(orient='records')]\n",
    "    print(bool_list)\n",
    "\n",
    "f,d = random_forest(df,5)\n",
    "bootstrap(f,d)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7dbb88c84ecda04e2dfa8176cb6708abf8c95c32934f510df16d1a1f4945387"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
